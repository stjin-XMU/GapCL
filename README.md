# GapCL
This is the repository for *Adaptive Symmetric Adversarial Perturbation Augmentation for Molecular Graph Representations*
We design  a integrating **G**radient-based **a**dversarial **p**erturbations with dual-fusion enhanced **C**ontrastive **L**earning methods (GapCL). It can be directly integrated into graph-based models, and significantly improving the capabilities of these models (e.g., GCN, GAT, MPNN, CoMPT, Uni-mol) for molecular representation learning.
# Abstract
High-quality molecular representation is essential for AI-driven drug discovery. Despite recent progress in Graph Neural Networks (GNNs) for this purpose, challenges such as data imbalance and overfitting persist due to the limited availability of labeled molecules. Augmentation techniques have become a popular solution, yet strategies that modify the topological structure of molecular graphs could lead to the loss of critical chemical information. Moreover, adversarial augmentation approaches, given the sparsity and complexity of molecular data, tend to amplify the potential risks of introducing noise. 
This paper introduces a novel plug-and-play architecture, GapCL, which employs a symmetric perturbation mechanism during gradient-based adversarial augmentation to ensure that the perturbed graphs retain potentially essential chemical space information. Additionally, GapCL incorporates a dual-fusion attention module to amplify key information and leverages contrastive learning constraints to enable an adaptive perturbation strategy tailored to various benchmark models. The proposed method has been evaluated across 12 molecular property prediction tasks, demonstrating the potential of GapCL to comprehensively enhance the robustness and generalization of molecular graph representation models. Further experimental studies also indicate that equipping models with GapCL leads to improved representation capabilities and achieves state-of-the-art performance in most cases. The source data and code are available at https://github.com/stjin-XMU/GapCL.git.
![GapCL model](https://github.com/stjin-XMU/GapCL/blob/main/GapCL.png)

# Environment
## GPU environment
CUDA 11.8

## create a new conda environment
- conda create -n GapCL python=3.8
- conda activate GapCL

## Requirements
- dgl==2.1.0+cu118
- dglgo==0.0.2
- networkx==3.1
- numpy==1.24.4
- ogb==1.3.6
- pandas==2.0.3
- pillow==10.4.0
- PyYAML==6.0.1
- rdkit==2022.9.1
- rdkit-pypi==2022.9.5
- requests==2.32.3
- scikit-learn==1.3.2
- scipy==1.10.1
- torch==2.2.1+cu118
- torchaudio==2.2.1+cu118
- torchdata
- torchvision==0.17.1+cu118
- tqdm==4.66.4
- deepchem==2.8.0

## install environment
This repositories is built based on python == 3.8.19. You could simply run

`pip install -r requirements.txt`

to install other packages.

# Datasets
We selected 12 subtasks from the MoleculeNet dataset ([Wu et al. 2018](10.1039/C7SC02664A)) for experimental evaluation, comprising 9 classification tasks and 3 regression tasks. 

| Dataset | #Molecule | #Task | #Task Type |
| :---: | :---: | :---: |:---: |
| BBBP  | 2,035 | 1 | Classification|
| BACE | 1,513 | 1 | Classification |
| HIV | 41,127 | 1 | Classification |
| Tox21 | 7,821 | 12 | Classification | 
| SIDER | 1,379 | 27 | Classification |
| ClinTox | 1,468 | 2 | Classification |
| ToxCast | 8615 | 617 | Classification |
| MUV | 93127 | 17 | Classification |
| PCBA | 437,928 | 128 | Classification |
| ESOL | 1,128 | 1 | Regression |
| FreeSolv | 642 | 1 | Regression |
| Lipophilicity  | 4,198 | 1 | Regression | 

# Baselines
We refer to some excellent implementations of baselines used in our paper.
## Graph representation models
- GCN ([Kipf and Welling 2016](https://doi.org/10.48550/arXiv.1609.02907))
  
  https://github.com/tkipf/gcn
  
  https://github.com/tkipf/pygcn
  
- GAT ([Velickovic et al. 2017](https://doi.org/10.48550/arXiv.1710.10903))
  
  https://github.com/Diego999/pyGAT
  
- MPNN ([Gilmer et al. 2017](https://arxiv.org/pdf/1704.01212))
  
  https://github.com/brain-research/mpnn
  
- CoMPT ([Chen et al.2021](https://doi.org/10.24963/ijcai.2021/309))
  
  https://github.com/jcchan23/CoMPT
  
- Uni-mol ([Zhou et al. 2023](https://openreview.net/forum?id=6K2RM6wVqKu))
  
  https://github.com/deepmodeling/Uni-Mol
  
## Adversarial learning methods
- PGD ([Madry et al. 2017](https://doi.org/10.48550/arXiv.1706.06083))
  
  https://github.com/Harry24k/PGD-pytorch
  
- FLAG ([kong2022robust](https://arxiv.org/abs/2010.09891))
  
  https://github.com/devnkong/FLAG
  
## Other comparison models
| Model | #Model Type | #Model | #Model Type |
| :---: | :---: | :---: |:---: |
| D-MPNN  | Supervised | Attentive FP | Supervised |
| N-gram  | Pretraining  | PretrainGNN | Pretraining |
| GROVER | Pretraining  |  GraphMVP | Pretraining  |
| MolCLR | Pretraining  | GEM | Pretraining  |

# ðŸŒŸQuick Run
First, you need to preprocess the molecular datasets to the format of Uni-mol. Then call train.py to reproduce the results. For instance, if you want to implement the domain adaptation task with GapCL in SIDER, you can use the following comment:

`python train.py ./configs/bbbp.json MPNN cuda:0 chem_scaffold`

# Reference
If our paper or code is helpful to you, please do not hesitate to point a star for our repository and cite the following content.



